---
layout: default
title: 'Describe Cluster hardware best practices'
base-url: home/11g-oce-rac/section-1---grid-infrastructure-clusterware-and-asm/oracle-grid-infrastructure/describe-cluster-hardware-best-practices.html
breadcrumbs:
- title: 'Home'
  url: home.html
- title: '11G OCE RAC'
  url: home/11g-oce-rac.html
- title: 'Section 1 - Grid Infrastructure: Clusterware and ASM'
  url: home/11g-oce-rac/section-1---grid-infrastructure-clusterware-and-asm.html
- title: 'Oracle Grid Infrastructure'
  url: home/11g-oce-rac/section-1---grid-infrastructure-clusterware-and-asm/oracle-grid-infrastructure.html
- title: 'Describe Cluster hardware best practices'
  url: home/11g-oce-rac/section-1---grid-infrastructure-clusterware-and-asm/oracle-grid-infrastructure/describe-cluster-hardware-best-practices.html
---
<div dir="ltr">
 <h2>
  <a name="TOC-Hardware-Design">
  </a>
  Hardware Design
 </h2>
 <p>
  In
  <a href="http://docs.oracle.com/cd/E11882_01/rac.112/e10717/toc.htm" rel="nofollow">
   Oracle® Clusterware Administration and Deployment Guide 11g Release 2 (11.2)
  </a>
  , the section,
  <a href="http://docs.oracle.com/cd/E11882_01/rac.112/e10717/intro.htm#BABBDGJI" rel="nofollow">
   Oracle Clusterware Hardware Concepts and Requirements
  </a>
  , says that:
 </p>
 <blockquote>
  <p>
   …However, a server that is part of a cluster, otherwise known as a node or a cluster member, requires a second network. This second network is referred to as the interconnect. For this reason, cluster member nodes require at least two network interface cards: one for a public network and one for a private network. The interconnect network is a private network using a switch (or multiple switches) that only the nodes in the cluster can access.
  </p>
  <p>
   …
  </p>
  <p>
   …If you are implementing a cluster for high availability, then configure redundancy for all of the components of the infrastructure as follows:
  </p>
  <ul>
   <li>
    At least two network interfaces for the public network, bonded to provide one address
   </li>
   <li>
    At least two network interfaces for the private interconnect network, also bonded to provide one address
   </li>
  </ul>
  <p>
   The cluster requires cluster-aware storage that is connected to each server in the cluster. This may also be referred to as a multihost device. Oracle Clusterware supports NFS, iSCSI, Direct Attached Storage (DAS), Storage Area Network (SAN) storage, and Network Attached Storage (NAS).
  </p>
  <p>
   To provide redundancy for storage, generally provide at least two connections from each server to the cluster-aware storage. There may be more connections depending on your I/O requirements. It is important to consider the I/O requirements of the entire cluster when choosing your storage subsystem.
  </p>
 </blockquote>
 <h2>
  <a name="TOC-IPMI">
  </a>
  IPMI
 </h2>
 <p>
  In
  <a href="http://docs.oracle.com/cd/E11882_01/install.112/e10812/toc.htm" rel="nofollow">
   Oracle® Grid Infrastructure Installation Guide 11g Release 2 (11.2) for Linux
  </a>
  , the section,
  <a href="http://docs.oracle.com/cd/E11882_01/install.112/e10812/prelinux.htm#BABJJHHI" rel="nofollow">
   2.13 Enabling Intelligent Platform Management Interface (IPMI)
  </a>
  , says that:
 </p>
 <blockquote>
  <p>
   Intelligent Platform Management Interface (IPMI) provides a set of common interfaces to computer hardware and firmware that system administrators can use to monitor system health and manage the system. With Oracle 11g release 2, Oracle Clusterware can integrate IPMI to provide failure isolation support and to ensure cluster integrity.
  </p>
  <p>
   You can configure node-termination with IPMI during installation by selecting a node-termination protocol, such as IPMI. You can also configure IPMI after installation with
   <span style="font-family:monospace">
    crsctl
   </span>
   commands.
  </p>
 </blockquote>
 <p>
  In
  <a href="http://docs.oracle.com/cd/E11882_01/rac.112/e10717/toc.htm" rel="nofollow">
   Oracle® Clusterware Administration and Deployment Guide 11g Release 2 (11.2)
  </a>
  , the section,
  <a href="http://docs.oracle.com/cd/E11882_01/rac.112/e10717/votocr.htm#BGBGAJEI" rel="nofollow">
   About Using IPMI for Failure Isolation
  </a>
  , says that:
 </p>
 <blockquote>
  <p>
   Failure isolation is a process by which a failed node is isolated from the rest of the cluster to prevent whatever happened to the failed node from happening to other nodes. You must configure and use an external mechanism capable of restarting a problem node without cooperation either from Oracle Clusterware or from the operating system running on that node. To provide this capability, Oracle Clusterware 11g release 2 (11.2) supports the Intelligent Management Platform Interface specification (IPMI), an industry-standard management protocol.
  </p>
 </blockquote>
</div>
