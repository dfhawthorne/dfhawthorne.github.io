---
layout: default
title: Richmond Cluster (16)
base-url: home/10g-ocm/10g-system-builds/richmond-cluster/richmond-cluster-16.html
breadcrumbs:
- title: Home
  url: home.html
- title: 10G OCM
  url: home/10g-ocm.html
- title: 10G System Builds
  url: home/10g-ocm/10g-system-builds.html
- title: Richmond Cluster
  url: home/10g-ocm/10g-system-builds/richmond-cluster.html
- title: Richmond Cluster (16)
  url: home/10g-ocm/10g-system-builds/richmond-cluster/richmond-cluster-16.html
scroll-bar:
  left-link:
    url: /home/10g-ocm/10g-system-builds/richmond-cluster/richmond-cluster-15.html
    title: Richmond Cluster (15)
  right-link:
    url: home/10g-ocm/10g-system-builds/richmond-cluster/richmond-cluster-17.html
    title: Richmond Cluster (17)
---
<div dir="ltr">
 <p style="FONT-SIZE:smaller;FONT-STYLE:italic">
  Friday 09 May, 2008 - 20:50
 </p>
 <p>
  Updated the kernel parameters (refer pp.2-37 to 2-38 of
  <a href="http://www.oracle.com/pls/db102/to_pdf?pathname=install.102?b14203.pdf&amp;remark=portal+%28Installation%29" rel="nofollow">
   Clusterware Installation for Linux
  </a>
  ):
  <br/>
  <code>
   $ su -
   <br/>
   # cat &gt;&gt;/etc/sysctl.conf
   <br/>
   kernel.sem = 250 32000 100 128
   <br/>
   kernel.shmmax = 526059520
   <br/>
   net.ipv4.ip_local_port_range = 1024 65000
   <br/>
   net.core.rmem_default = 1048576
   <br/>
   net.core.rmem_max = 1048576
   <br/>
   net.core.wmem_default = 262144
   <br/>
   net.core.wmem_max = 262144
  </code>
 </p>
 <p>
  Updated the shell limits for oracle user (refer p.2-38 of
  <a href="http://www.oracle.com/pls/db102/to_pdf?pathname=install.102?b14203.pdf&amp;remark=portal+%28Installation%29" rel="nofollow">
   Clusterware Installation for Linux
  </a>
  ):
  <br/>
  <code>
   $ su -
   <br/>
   # cat &gt;&gt;/etc/security/limits.conf
   <br/>
   oracle soft nproc 2047
   <br/>
   oracle hard nproc 16384
   <br/>
   oracle soft nofile 1024
   <br/>
   oracle hard nofile 65536
   <br/>
   <br/>
   # cat &gt;&gt;/etc/pam.d/login
   <br/>
   session required pam_limits.so
   <br/>
  </code>
 </p>
 <p>
  Updated the security on the raw devices for OCR (refer pp.3-15 to 3-16 of
  <a href="http://www.oracle.com/pls/db102/to_pdf?pathname=install.102?b14203.pdf&amp;remark=portal+%28Installation%29" rel="nofollow">
   Clusterware Installation for Linux
  </a>
  ):
  <br/>
  <code>
   $ su -
   <br/>
   # chown root:oinstall /dev/raw/raw1 /dev/raw/raw16
   <br/>
   # chmod 640 /dev/raw/raw1 /dev/raw/raw16
  </code>
 </p>
 <p>
  Updated the security on the raw devices for voting disks (refer pp.3-35 of
  <a href="http://www.oracle.com/pls/db102/to_pdf?pathname=install.102?b14203.pdf&amp;remark=portal+%28Installation%29" rel="nofollow">
   Clusterware Installation for Linux
  </a>
  ):
  <br/>
  <code>
   $ su -
   <br/>
   # chown oracle:oinstall /dev/raw/raw2 /dev/raw/raw17 /dev/raw/raw32
   <br/>
   # chmod 640 /dev/raw/raw2 /dev/raw/raw17 /dev/raw/raw32
  </code>
 </p>
 <p>
  Rebooted
  <b>
   richmond1
  </b>
 </p>
 <p>
  Ran OUI for one (1) node only (
  <b>
   richmond1
  </b>
  ). When I ran the root scripts, I got the following messages:
  <br/>
  <code>
   # /u00/app/oracle/oraInventory/orainstRoot.sh
   <br/>
   Changing permissions of /u00/app/oracle/oraInventory to 770.
   <br/>
   Changing groupname of /u00/app/oracle/oraInventory to oinstall.
   <br/>
   The execution of the script is complete
   <br/>
   # /u00/crs/oracle/product/10/app/root.sh
   <br/>
   Checking to see if Oracle CRS stack is already configured
   <br/>
   /etc/oracle does not exist. Creating it now.
   <br/>
   <br/>
   Setting the permissions on OCR backup directory
   <br/>
   Setting up NS directories
   <br/>
   Oracle Cluster Registry configuration upgraded successfully
   <br/>
   assigning default hostname richmond1 for node 1.
   <br/>
   Successfully accumulated necessary OCR keys.
   <br/>
   Using ports: CSS=49895 CRS=49896 EVMC=49898 and EVMR=49897.
   <br/>
   node :
   <br/>
   node 1: richmond1 richmond1-priv richmond1
   <br/>
   Creating OCR keys for user 'root', privgrp 'root'..
   <br/>
   Operation successful.
   <br/>
   Now formatting voting device: /dev/raw/raw2
   <br/>
   Now formatting voting device: /dev/raw/raw17
   <br/>
   Now formatting voting device: /dev/raw/raw32
   <br/>
   Format of 3 voting devices complete.
   <br/>
   Startup will be queued to init within 90 seconds.
   <br/>
   Adding daemons to inittab
   <br/>
   Expecting the CRS daemons to be up within 600 seconds.
   <br/>
   CSS is active on these nodes.
   <br/>
   richmond1
   <br/>
   CSS is active on all nodes.
   <br/>
   Waiting for the Oracle CRSD and EVMD to start
   <br/>
   Oracle CRS stack installed and running under init(1M)
   <br/>
   Running vipca(silent) for configuring nodeapps
   <br/>
   The given interface(s), "eth0" is not public. Public interfaces should be used to configure virtual IPs.
  </code>
 </p>
 <p>
  Checking the status of the CRS stack:
  <br/>
  <code>
   # /u00/crs/oracle/product/10/app/bin/crsctl check crs
   <br/>
   CSS appears healthy
   <br/>
   CRS appears healthy
   <br/>
   EVM appears healthy
  </code>
 </p>
 <p>
  Clicked OK in OUI to continue, and the post-installation verification failed - presumably because of the VIP.
 </p>
 <p>
  Ran
  <code>
   /u00/crs/oracle/product/10/app/bin/vipca
  </code>
  as
  <b>
   root
  </b>
  . Got the following messages:
  <br/>
  <code>
   CRS-1006: No more members to consider
   <br/>
   CRS-0215: Could not start resource 'ora.richmond1.vip'
  </code>
 </p>
 <p>
  The logs in
  <code>
   /u00/crs/oracle/product/10/app/log/client
  </code>
  are next to useless. I cannot find any explanatory message why this resource cannot come online.
 </p>
 <p>
  Although this resource is not online, the installation is deemed to be successful when the installation verification is retried in OUI. Apparrently, cluvfy just checks for the existence of the resource, not caring if it is online or not.
 </p>
</div>
